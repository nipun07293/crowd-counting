{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3xaRsxjCFUB",
        "outputId": "5b868fa2-5456-4981-f01d-802a21b2ecad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "\n",
        "train_images_dir = '/content/drive/MyDrive/images'\n",
        "train_gt_dir = '/content/drive/MyDrive/ground_truth'\n",
        "train_dm_dir = '/content/drive/MyDrive/density_maps'\n",
        "\n",
        "test_images_dir = '/content/drive/MyDrive/images1'\n",
        "test_gt_dir = '/content/drive/MyDrive/ground_truth1'\n",
        "test_dm_dir = '/content/drive/MyDrive/density_maps1'\n",
        "\n",
        "viz_dir = '/content/drive/MyDrive/cctrans_visualizations'\n",
        "MODEL_SAVE_DIR = '/content/drive/MyDrive/cctrans_checkpoints'\n",
        "PRETRAINED_WEIGHTS_PATH = '/content/drive/MyDrive/64_256_upsampler.pt'\n",
        "\n",
        "os.makedirs(train_dm_dir, exist_ok=True)\n",
        "os.makedirs(test_dm_dir, exist_ok=True)\n",
        "os.makedirs(viz_dir, exist_ok=True)\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
        "\n",
        "class CrowdDataset(Dataset):\n",
        "    def __init__(self, images_dir, gt_dir, dm_dir, image_size=(256, 256)):\n",
        "        self.images_dir = images_dir\n",
        "        self.gt_dir = gt_dir\n",
        "        self.dm_dir = dm_dir\n",
        "        self.image_size = image_size\n",
        "        self.image_files = sorted([f for f in os.listdir(images_dir) if f.endswith(('.jpg', '.png'))])\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        import numpy as np\n",
        "        import scipy.io as sio\n",
        "\n",
        "        img_name = self.image_files[idx]\n",
        "        img_path = os.path.join(self.images_dir, img_name)\n",
        "\n",
        "        base_name_parts = os.path.splitext(img_name)[0].split('_')\n",
        "        if len(base_name_parts) > 1 and base_name_parts[0] == 'IMG':\n",
        "            img_number = base_name_parts[1]\n",
        "            gt_name = f'GT_IMG_{img_number}.mat'\n",
        "        else:\n",
        "            base_name = os.path.splitext(img_name)[0]\n",
        "            gt_name = f'GT_{base_name}.mat'\n",
        "        gt_path = os.path.join(self.gt_dir, gt_name)\n",
        "\n",
        "        dm_path = os.path.join(self.dm_dir, f'{os.path.splitext(img_name)[0]}.npy')\n",
        "\n",
        "        image = cv2.imread(img_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        original_size = image.shape[:2]\n",
        "        image_resized = cv2.resize(image, self.image_size)\n",
        "\n",
        "        image_tensor = self.transform(Image.fromarray(image_resized))\n",
        "\n",
        "        if os.path.exists(dm_path):\n",
        "            density_map = np.load(dm_path)\n",
        "            if density_map.shape[:2] != self.image_size:\n",
        "                density_map = cv2.resize(density_map, self.image_size, interpolation=cv2.INTER_LINEAR)\n",
        "            crowd_count = np.sum(density_map)\n",
        "        else:\n",
        "            points = np.array([])\n",
        "            try:\n",
        "                gt_data = sio.loadmat(gt_path)\n",
        "                points = gt_data['image_info'][0, 0][0][0][0]\n",
        "            except:\n",
        "                points = np.array([])\n",
        "\n",
        "            if points.size > 0:\n",
        "                points_resized = points.copy()\n",
        "                points_resized[:, 0] = points[:, 0] * (self.image_size[1] / original_size[1])\n",
        "                points_resized[:, 1] = points[:, 1] * (self.image_size[0] / original_size[0])\n",
        "                crowd_count = points.shape[0]\n",
        "            else:\n",
        "                points_resized = np.array([])\n",
        "                crowd_count = 0\n",
        "\n",
        "            density_map = self.create_density_map(points_resized, target_size=self.image_size)\n",
        "            np.save(dm_path, density_map)\n",
        "\n",
        "        density_map_tensor = torch.from_numpy(density_map).float().unsqueeze(0)\n",
        "        return image_tensor, density_map_tensor, crowd_count\n",
        "\n",
        "    def create_density_map(self, points, target_size):\n",
        "        h, w = target_size\n",
        "        density_map = np.zeros((h, w), dtype=np.float32)\n",
        "        if points.shape[0] == 0:\n",
        "            return density_map\n",
        "\n",
        "        sigma = 8.0\n",
        "        for i in range(points.shape[0]):\n",
        "            x = int(points[i, 0])\n",
        "            y = int(points[i, 1])\n",
        "            if 0 <= x < w and 0 <= y < h:\n",
        "                density_map[y, x] = 1.0\n",
        "\n",
        "        density_map = gaussian_filter(density_map, sigma=sigma, mode='constant')\n",
        "        current_sum = np.sum(density_map)\n",
        "        if current_sum > 0:\n",
        "            density_map = density_map / current_sum * points.shape[0]\n",
        "        return density_map\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class Down(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "class Up(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
        "        super().__init__()\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512, bilinear)\n",
        "        self.up2 = Up(512, 256, bilinear)\n",
        "        self.up3 = Up(256, 128, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        return self.outc(x)\n",
        "\n",
        "class CrowdDiff(nn.Module):\n",
        "    def __init__(self, img_channels=3, output_channels=1, time_embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.denoising_unet = UNet(n_channels=img_channels, n_classes=output_channels)\n",
        "        self.counting_branch = nn.Sequential(\n",
        "            nn.Conv2d(output_channels, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        density_map = F.relu(self.denoising_unet(x))\n",
        "        predicted_count = torch.sum(density_map, dim=[1, 2, 3])\n",
        "        return density_map, predicted_count\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    criterion = nn.MSELoss(reduction='sum')\n",
        "    for images, gt_dms, _ in tqdm(dataloader, desc=\"Training\"):\n",
        "        images = images.to(device)\n",
        "        gt_dms = gt_dms.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        pred_dm, _ = model(images)\n",
        "        if pred_dm.shape[2:] != gt_dms.shape[2:]:\n",
        "            gt_dms = F.interpolate(gt_dms, size=pred_dm.shape[2:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(pred_dm, gt_dms) / images.shape[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    mae, mse = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, _, gt_counts in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            images = images.to(device)\n",
        "            _, pred_counts = model(images)\n",
        "            mae += torch.abs(pred_counts - gt_counts.to(device)).sum().item()\n",
        "            mse += ((pred_counts - gt_counts.to(device))**2).sum().item()\n",
        "    n = len(dataloader.dataset)\n",
        "    return mae/n, (mse/n)**0.5\n",
        "\n",
        "def visualize_predictions(model, dataloader, device, num_images=5):\n",
        "    model.eval()\n",
        "    inv_normalize = transforms.Normalize(\n",
        "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
        "        std=[1/0.229, 1/0.224, 1/0.225]\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        for i, (images, gt_dm, _) in enumerate(dataloader):\n",
        "            if i >= num_images: break\n",
        "            images = images.to(device)\n",
        "            pred_dm, _ = model(images)\n",
        "            img_np = inv_normalize(images[0].cpu()).permute(1,2,0).numpy().clip(0,1)\n",
        "            gt = gt_dm.to(device)\n",
        "            if pred_dm.shape[2:] != gt.shape[2:]:\n",
        "                gt = F.interpolate(gt, size=pred_dm.shape[2:], mode='bilinear', align_corners=False)\n",
        "            gt_np = gt[0].squeeze().cpu().numpy()\n",
        "            pred_np = pred_dm[0].squeeze().cpu().numpy()\n",
        "            fig, axes = plt.subplots(1,3,figsize=(18,6))\n",
        "            axes[0].imshow(img_np); axes[0].axis('off')\n",
        "            axes[1].imshow(gt_np, cmap='jet'); axes[1].axis('off')\n",
        "            axes[2].imshow(pred_np, cmap='jet'); axes[2].axis('off')\n",
        "            plt.savefig(os.path.join(viz_dir, f'vis_{i}.png'))\n",
        "            plt.close(fig)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    train_dataset = CrowdDataset(train_images_dir, train_gt_dir, train_dm_dir)\n",
        "    test_dataset = CrowdDataset(test_images_dir, test_gt_dir, test_dm_dir)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = CrowdDiff(img_channels=3, output_channels=1).to(device)\n",
        "    if os.path.exists(PRETRAINED_WEIGHTS_PATH):\n",
        "        state = torch.load(PRETRAINED_WEIGHTS_PATH, map_location=device)\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained = {k:v for k,v in state.items() if k in model_dict and v.shape==model_dict[k].shape}\n",
        "        model_dict.update(pretrained)\n",
        "        model.load_state_dict(model_dict)\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
        "    best_mae = float('inf')\n",
        "    for epoch in range(20):\n",
        "        print(f\"Epoch {epoch+1}/20\")\n",
        "        loss = train_one_epoch(model, train_loader, optimizer, device)\n",
        "        print(f\"Loss: {loss:.4f}\")\n",
        "        mae, rmse = evaluate(model, test_loader, device)\n",
        "        print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
        "        if mae < best_mae:\n",
        "            best_mae = mae\n",
        "            torch.save(model.state_dict(), os.path.join(MODEL_SAVE_DIR, 'best.pth'))\n",
        "    visualize_predictions(model, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEGbqolyZwga",
        "outputId": "4d96129b-cd25-4ad5-f103-fd42d33ef16a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load pre-trained weights from /content/drive/MyDrive/64_256_upsampler.pt\n",
            "\n",
            "--- State Dictionary Loading Summary ---\n",
            "Warning: Missing key(s) in pre-trained weights (will be initialized randomly): 122 keys, e.g., ['denoising_unet.up1.conv.double_conv.4.bias', 'denoising_unet.down4.maxpool_conv.1.double_conv.1.running_var', 'denoising_unet.down1.maxpool_conv.1.double_conv.1.weight', 'denoising_unet.inc.double_conv.1.num_batches_tracked', 'denoising_unet.up3.conv.double_conv.1.running_var']...\n",
            "Warning: Unexpected key(s) in pre-trained weights (ignored): 567 keys, e.g., ['input_blocks.4.0.emb_layers.1.weight', 'output_blocks.11.0.out_layers.3.weight', 'input_blocks.13.0.emb_layers.1.bias', 'input_blocks.10.0.out_layers.3.weight', 'input_blocks.10.0.in_layers.2.weight']...\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "--- Epoch 1/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [02:20<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 6054.9362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [03:10<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 1: MAE = 5231.85, RMSE = 5267.91\n",
            "Saved best model with MAE: 5231.85\n",
            "\n",
            "--- Epoch 2/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:29<00:00,  3.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 205.8007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 2: MAE = 576.33, RMSE = 590.17\n",
            "Saved best model with MAE: 576.33\n",
            "\n",
            "--- Epoch 3/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:32<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 43.7598\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 3: MAE = 116.98, RMSE = 131.51\n",
            "Saved best model with MAE: 116.98\n",
            "\n",
            "--- Epoch 4/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:31<00:00,  3.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 25.5400\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 4: MAE = 67.66, RMSE = 91.99\n",
            "Saved best model with MAE: 67.66\n",
            "\n",
            "--- Epoch 5/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:31<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 20.1546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 5: MAE = 70.87, RMSE = 109.81\n",
            "\n",
            "--- Epoch 6/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 17.8188\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 6: MAE = 74.26, RMSE = 114.03\n",
            "\n",
            "--- Epoch 7/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 16.2063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 7: MAE = 83.15, RMSE = 122.79\n",
            "\n",
            "--- Epoch 8/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 15.5387\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 8: MAE = 91.82, RMSE = 130.20\n",
            "\n",
            "--- Epoch 9/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 14.7486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 9: MAE = 100.35, RMSE = 136.91\n",
            "\n",
            "--- Epoch 10/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 14.6452\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 10: MAE = 98.26, RMSE = 135.30\n",
            "\n",
            "--- Epoch 11/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 14.1444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 11: MAE = 104.75, RMSE = 140.46\n",
            "\n",
            "--- Epoch 12/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.8696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 12: MAE = 109.90, RMSE = 144.51\n",
            "\n",
            "--- Epoch 13/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 14.4803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 13: MAE = 104.63, RMSE = 140.33\n",
            "\n",
            "--- Epoch 14/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.9236\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 29.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 14: MAE = 107.95, RMSE = 142.79\n",
            "\n",
            "--- Epoch 15/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.6047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 15: MAE = 113.31, RMSE = 147.06\n",
            "\n",
            "--- Epoch 16/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.4131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 16: MAE = 114.07, RMSE = 147.71\n",
            "\n",
            "--- Epoch 17/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.3334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 17: MAE = 117.28, RMSE = 150.28\n",
            "\n",
            "--- Epoch 18/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.2437\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 18: MAE = 117.02, RMSE = 150.13\n",
            "\n",
            "--- Epoch 19/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.1935\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 19: MAE = 118.58, RMSE = 151.35\n",
            "\n",
            "--- Epoch 20/20 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 100/100 [00:28<00:00,  3.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Training Loss: 13.1575\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation after epoch 20: MAE = 118.99, RMSE = 151.71\n",
            "\n",
            "--- Final Evaluation ---\n",
            "Loading best model for final evaluation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 316/316 [00:10<00:00, 30.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test MAE (Best Model): 67.66\n",
            "Final Test RMSE (Best Model): 91.99\n",
            "\n",
            "--- Generating Visualizations ---\n",
            "Visualizations saved to: /content/drive/MyDrive/cctrans_visualizations\n"
          ]
        }
      ]
    }
  ]
}